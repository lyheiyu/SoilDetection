import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
# 1ï¸âƒ£ åŠ è½½ LLaMA æ¨¡å‹
# âœ… 2ï¸âƒ£ è¯»å–åœŸå£¤ pH æ•°æ®
json_file = "ireland_soil_ph_data.json"
with open(json_file, "r") as f:
    soil_data = json.load(f)

# âœ… 3ï¸âƒ£ å®šä¹‰æŸ¥è¯¢å‡½æ•°
def get_soil_ph(latitude, longitude):
    """ä» JSON æ–‡ä»¶æŸ¥è¯¢ pH å€¼"""
    for entry in soil_data:
        if round(entry["latitude"], 3) == round(latitude, 3) and round(entry["longitude"], 3) == round(longitude, 3):
            return entry["ph_h2o"]
    return "Data not found"

model_name = "./function_call3"  # ä½ å¯ä»¥ä½¿ç”¨æœ¬åœ°è·¯å¾„
tokenizer = AutoTokenizer.from_pretrained(model_name)
from transformers import BitsAndBytesConfig
bnb_config = BitsAndBytesConfig(load_in_8bit=True)
# model = AutoModelForCausalLM.from_pretrained(
#   "./llama-3-12b-local/",
#     # device_map={"": "cpu"},
#     device_map="auto",
#     offload_folder="offload",
#     quantization_config=bnb_config,
#     # no_split_module_classes=["LlamaDecoderLayer"]
# )
base_model_path = "./Meta_Llama_3_8B2"  # ä¿®æ”¹ä¸ºä½ çš„å®é™…æ¨¡å‹è·¯å¾„
# finetuned_model_path = "./llama3_lora_finetuned"
# finetuned_model_path = "./lora_output3"

finetuned_model_path = "./function_call3"
# åŠ è½½åŸºç¡€æ¨¡å‹
print("Loading base model...")
# model = AutoModelForCausalLM.from_pretrained(
#     base_model_path,
#     device_map="auto"  # è‡ªåŠ¨åˆ†é…è®¾å¤‡
# )
model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    # device_map={"": "cpu"},
    device_map="auto",
    offload_folder="offload",
    quantization_config=bnb_config,
    # no_split_module_classes=["LlamaDecoderLayer"]
)

print("Loading fine-tuned LoRA weights...")
model = PeftModel.from_pretrained(model, finetuned_model_path, is_trainable=False)

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path)
# åŠ è½½åˆ†è¯å™¨
# tokenizer = AutoTokenizer.from_pretrained(base_model_path)
print(model.peft_config["default"])

# 2ï¸âƒ£ åŠ è½½åœŸå£¤æ•°æ®
json_file = "ireland_soil_ph_data.json"
with open(json_file, "r") as f:
    soil_data = json.load(f)


# 3ï¸âƒ£ æŸ¥è¯¢ pH å€¼çš„å‡½æ•°
def get_ph_value(latitude, longitude):
    """ä» JSON æ–‡ä»¶æŸ¥è¯¢ pH å€¼"""
    for entry in soil_data:
        if round(entry["latitude"], 3) == round(latitude, 3) and round(entry["longitude"], 3) == round(longitude, 3):
            return entry["ph_h2o"]
    return "Data not found"

def clean_llama_output(output_text):
    """æ¸…ç† LLaMA ç”Ÿæˆçš„é JSON ä»£ç """
    if "{" in output_text:
        output_text = output_text[output_text.index("{"):]  # ä¿ç•™ä» `{` å¼€å§‹çš„éƒ¨åˆ†
    if "}" in output_text:
        output_text = output_text[:output_text.rindex("}")+1]  # ä¿ç•™åˆ° `}` ç»“æŸ
    return output_text.strip()
# 4ï¸âƒ£ è®© LLaMA ç”Ÿæˆ JSON æ ¼å¼çš„å‡½æ•°è°ƒç”¨
def llama_generate_function_call(prompt):
    """ä½¿ç”¨ LLaMA è§£æ JSON å¹¶è¿”å›"""
    input_text = f"""


    User request: "{prompt}"

    Function call (JSON only):
    """

    # âœ… ç¡®ä¿ tokenizer å…·æœ‰ `pad_token`
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # âœ… Tokenize è¾“å…¥
    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=254  # é™åˆ¶é•¿åº¦ï¼Œé¿å…è¾“å‡ºè¢«æˆªæ–­
    )

    input_ids = inputs["input_ids"].to(model.device)
    attention_mask = inputs["attention_mask"].to(model.device)

    # âœ… è®¾ç½® pad_token_id
    model.config.pad_token_id = tokenizer.pad_token_id

    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            attention_mask=attention_mask,  # âœ… è§£å†³ `attention_mask` ç›¸å…³è­¦å‘Š
            max_length=256,  # âœ… é™åˆ¶ç”Ÿæˆé•¿åº¦ï¼Œé˜²æ­¢ JSON ä¸å®Œæ•´
            temperature=0.5,
            do_sample=False,  # âœ… å…³é—­éšæœºé‡‡æ ·ï¼Œç¡®ä¿è¾“å‡ºç¨³å®š
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
        )

    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    output_text = clean_llama_output(output_text)
    print(output_text)
    # âœ… è§£æ JSON
    try:
        function_call = json.loads(output_text.strip())
        print(function_call)
        return function_call
    except json.JSONDecodeError:
        print("âš ï¸ LLaMA ç”Ÿæˆçš„è¾“å‡ºæ— æ³•è§£æä¸º JSONï¼")
        print("ğŸ” ç”Ÿæˆçš„å†…å®¹:", output_text)
        return {"error": "Invalid function call format"}


# 5ï¸âƒ£ å¤„ç† LLaMA ç”Ÿæˆçš„å‡½æ•°è°ƒç”¨
def process_function_call(prompt):
    """è§£æ LLaMA ç”Ÿæˆçš„ JSONï¼Œå¹¶æ‰§è¡ŒæŸ¥è¯¢"""
    function_call = llama_generate_function_call(prompt)

    if "error" in function_call:
        return function_call["error"]

    if function_call["name"] == "get_soil_ph":
        args = function_call["arguments"]
        lat, lon = args["latitude"], args["longitude"]
        ph_value = get_soil_ph(lat, lon)
        return f"The soil pH at latitude {lat}, longitude {lon} is: {ph_value}"

    return "Function call not recognized."


# 6ï¸âƒ£ äº¤äº’æ¨¡å¼
while True:
    user_input = input("Enter coordinates (e.g., 'What is the soil pH at latitude 53.3 longitude -6.2?'): ")
    print(1)
    if user_input.lower() in ["exit", "quit"]:
        break
    print(process_function_call(user_input))
